---
layout: post
title:  "Week 4: keyboards matias, sqlite, postmortem ddos, alerts thresholds honeycomb, deploy automation devtools"
---

* [Wireless Aluminum Tenkeyless Keyboard - Space Gray](https://matias.store/products/fk408btb): Apple looking keyboard with larger arrow keys, dedicated page up page down, long battery life, and can pair with 3 devices.
* [SQLite or PostgreSQL? It's Complicated!](https://www.twilio.com/blog/sqlite-postgresql-complicated): The story of a program with state stored in sqlite. It worked for awhile, until the amount of data and number of users outgrew the initial system as designed and so it was migrated. Could it have been made more efficient? Unsure. Sqlite is an amazing little tool that's perfect for many applications. Postgres is pretty great too! :)
* [SourceHut network outage post-mortem](https://sourcehut.org/blog/2024-01-19-outage-post-mortem/): Ddos attack and how sourcehut responded. Nice writeup. I thought the most basic cloudflare plans included ddos protection. Hrm ...
* [Alerts Are Fundamentally Messy](https://www.honeycomb.io/blog/alerts-are-fundamentally-messy): Finding balance between too noisy and the potential to miss interesting production events is a continuous process. Along with finding signals that matter in an evolving system.
* [The Scary Thing About Automating Deploys](https://slack.engineering/the-scary-thing-about-automating-deploys/?ck_subscriber_id=185275687): The Slack webapp is a monolith that is deployed regularly. They automated it and look for anomalous metrics to decide if a release is sane. Before that humans did it with tools and whatever feedback was available. There was a level of anxiety being the person pushing the button because it's not something that any 1 person does often (shared responsibility across the dev team) and the complexity of the system made it hard to tell whether signals were ok or not. 
* [Slackâ€™s Migration to a Cellular Architecture](https://slack.engineering/slacks-migration-to-a-cellular-architecture/): Slack reviewed network connections between data centers after an aws regional outage (us-east was there any doubt?) caused errors visible to end users.
* [Scaling Laravel to 100M+ jobs and 30,000 requests/sec](https://mateusguimaraes.com/posts/scaling-laravel): A message sending app hit a scaling ceiling at about 1-4mil messages per day and was optimized to send 100mil+ per day. The business requirements were studied to avoid or defer db work. In the end, there was a bunch of data copying that was avoided and per transaction limits added. Nice writeup and architecture re-think.