---
layout: post
title: "Weekly notes"
---

* [Prompt caching: 10x cheaper LLM tokens, but how?](https://ngrok.com/blog/prompt-caching/): A lot of compute is waste when generating output token by token. Llms work by feeding the same prompt into the model over and over again but 1 token longer than the last time and each time doing the same amount of work + a bit more for the next token.