<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<!-- saved from url=(0066)https://www.tbray.org/ongoing/When/202x/2021/05/15/Testing-in-2021 -->
<html xmlns:og="https://ogp.me/ns#" lang="en" class="wf-fftisawebpro-n4-active wf-fftisawebpro-n7-active wf-fftisawebpro-i4-active wf-active"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>ongoing by Tim Bray · Testing in the Twenties</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
<meta property="og:site_name" content="ongoing by Tim Bray">
<meta property="og:title" content="Testing in the Twenties">
<meta property="og:image" content="/ongoing/misc/podcast-default.jpg">
<meta property="og:type" content="website">

<link rel="stylesheet" type="text/css" media="screen" title="serif" href="./ongoing by Tim Bray · Testing in the Twenties_files/serif.css">
<script type="text/javascript" src="./ongoing by Tim Bray · Testing in the Twenties_files/ugm7uwx.js"></script>
<style type="text/css">#centercontent form,#centercontent li,#centercontent p,#commentHere p,#footer p,.caption > p,.employ,.ofp,.os,div.resume-payload{font-family:"ff-tisa-web-pro",serif;}</style><style type="text/css">@font-face{font-family:ff-tisa-web-pro;src:url(https://use.typekit.net/af/61961a/00000000000000000001707c/27/l?subset_id=2&fvd=n7&v=3) format("woff2"),url(https://use.typekit.net/af/61961a/00000000000000000001707c/27/d?subset_id=2&fvd=n7&v=3) format("woff"),url(https://use.typekit.net/af/61961a/00000000000000000001707c/27/a?subset_id=2&fvd=n7&v=3) format("opentype");font-weight:700;font-style:normal;font-stretch:normal;font-display:auto;}@font-face{font-family:ff-tisa-web-pro;src:url(https://use.typekit.net/af/6a3278/000000000000000000017080/27/l?subset_id=2&fvd=i4&v=3) format("woff2"),url(https://use.typekit.net/af/6a3278/000000000000000000017080/27/d?subset_id=2&fvd=i4&v=3) format("woff"),url(https://use.typekit.net/af/6a3278/000000000000000000017080/27/a?subset_id=2&fvd=i4&v=3) format("opentype");font-weight:400;font-style:italic;font-stretch:normal;font-display:auto;}@font-face{font-family:ff-tisa-web-pro;src:url(https://use.typekit.net/af/d3823a/000000000000000000017087/27/l?subset_id=2&fvd=n4&v=3) format("woff2"),url(https://use.typekit.net/af/d3823a/000000000000000000017087/27/d?subset_id=2&fvd=n4&v=3) format("woff"),url(https://use.typekit.net/af/d3823a/000000000000000000017087/27/a?subset_id=2&fvd=n4&v=3) format("opentype");font-weight:400;font-style:normal;font-stretch:normal;font-display:auto;}</style><script type="text/javascript">try{Typekit.load();}catch(e){}</script>
<script type="text/javascript" src="./ongoing by Tim Bray · Testing in the Twenties_files/ongoing.js"></script>
<link rel="alternate" type="application/atom+xml" title="Atom (full content)" href="https://www.tbray.org/ongoing/ongoing.atom">
<!-- Generated from XML source code using Perl, Expat, Emacs, Mysql, Ruby, Java, and ImageMagick.  Industrial-strength technology, baby. -->
</head><body itemscope="" itemtype="http://schema.org/Blog">
<div id="payload">
<div id="banner"><h1 itemprop="name">Testing in the Twenties</h1><div id="search"><form action="https://www.google.com/search" target="_parent">Search <input size="20" name="as_q"><input type="hidden" name="hl" value="en"><input type="hidden" name="ie" value="UTF-8"><input type="hidden" name="btnG" value="Google+Search"><input type="hidden" name="as_qdr" value="all"><input type="hidden" name="as_occt" value="any"><input type="hidden" name="as_dt" value="i"><input type="hidden" name="as_sitesearch" value="tbray.org"></form></div></div>
<div id="center-and-right"><div id="centercontent">
    <p><i>[This fragment is available in <a href="https://www.tbray.org/ongoing/When/202x/2021/05/15/Testing-in-2021.mp3">an audio version</a>.]</i></p>
<p itemprop="description">Grown-up software developers know perfectly well that testing is important. But<span class="dashes"> —</span> speaking
    here from experience<span class="dashes"> —</span> many aren’t doing enough. So I’m here to bang the testing
    drum, which our profession shouldn’t need to hear but apparently does.</p>

    <p>This was provoked by two Twitter threads
    (<a href="https://twitter.com/searls/status/1393571227650908162">here</a> and
    <a href="https://twitter.com/searls/status/1393385209089990659">here</a>) from
    <a href="https://twitter.com/searls">Justin Searls</a>, from which a couple of quotes: “almost all the advice you hear about
    software testing is bad. It’s either bad on its face or it leads to bad outcomes or it distracts by focusing on the wrong thing
    (usually tools)” and  
    “Nearly zero teams write expressive tests
    that establish clear boundaries, run quickly &amp; reliably, and only fail for useful reasons. Focus on that instead.”
    [Note: Justin apparently is
    <a href="https://testdouble.com/">in the testing business</a>.]</p>

    <p>Twitter threads twist and fork and are hard to follow, so I’m going to reach in and reproduce a couple of image grabs from
    one branch.</p>

    <div class="misc"><a href="https://www.tbray.org/ongoing/When/202x/2021/05/15/-big/dodds.jpg.html"><img alt="Picture credited to Dodds" title="Picture credited to Dodds" class="norm" src="./ongoing by Tim Bray · Testing in the Twenties_files/dodds.png"></a></div>
<div class="misc">· · ·</div>
<div class="main-image"><a href="https://www.tbray.org/ongoing/When/202x/2021/05/15/-big/spotify.jpg.html"><img alt="Credited to Spotify" title="Credited to Spotify" class="norm" src="./ongoing by Tim Bray · Testing in the Twenties_files/spotify.png"></a></div>
<p>Let me put a stake in the ground: I think those misshapen blobs are seriously wrong in important ways.</p>

    <p id="p-1" class="p1"><span class="h2">My prejudices</span> · 
    I’ve been doing software for money since 1979 and while it’s perfectly possible that I’m wrong, it’s not for lack of
    experience. Having said that, almost all my meaningful work has been low-level infrastructural stuff: Parsers, message routers,
    data viz frameworks, Web crawlers, full-text search.  So it’s possible that some of my findings are less true once you get out
    of the infrastructure space.<a href="https://www.tbray.org/ongoing/When/202x/2021/05/15/Testing-in-2021#p-1" class="plink">&nbsp;¶</a></p>

    <p id="p-2" class="p1"><span class="h2">History</span> · 
    In the first twenty years of my programming life, say up till the turn of the millennium, there was shockingly little
    software testing in the mainstream.  One result was, to quote
    <a href="https://en.wikipedia.org/wiki/Gerald_Weinberg">Gerald Weinberg</a>’s often-repeated crack, “If builders built
    buildings the way programmers wrote programs, then the first woodpecker that came along would destroy civilization.”<a href="https://www.tbray.org/ongoing/When/202x/2021/05/15/Testing-in-2021#p-2" class="plink">&nbsp;¶</a></p>

    <p>Back then it seemed that for any piece of software I wrote, after a
    couple of years I started hating it, because it became increasingly brittle and terrifying.  Looking back in the
    rear-view, I’m thinking I was reacting to the experience, common with untested code, of small changes
    unexpectedly causing large breakages for reasons that are hard to understand.</p>

    <p>Sometime in the first decade of this millennium, the needle moved.  My perception is that the initial impetus came at least
    partly out of the Ruby community, accelerated by the rise of Rails. 
    I started to hear the term “test-infected”, and I noticed that code submissions were apt to be coldly rejected if they weren’t
    accompanied by decent unit tests.</p>

    <p>Others have told me they initially got test-infected by the conversation around
    <a href="https://martinfowler.com/books/refactoring.html">Martin Fowler’s <cite>Refactoring</cite> book</a>, originally from
    1999, which made the point that you can’t really refactor untested code.</p>

    <p>In particular I remember attending the
    <a href="https://johntopley.com/2010/03/30/the-scottish-ruby-conference-2010/">Scottish Ruby Conference in 2010</a> and it
    seemed like more or less half the presentations were on testing best-practices and technology. I learned lessons there that I’m still
    using today.</p>

    <p>I’m pretty convinced that the biggest single contributor to improved software in my lifetime
    wasn’t object-orientation or higher-level languages or
    functional programming or strong typing or MVC or anything else: It was the rise of testing culture.</p>

    <p id="p-4" class="p1"><span class="h2">What I believe</span> · 
    The way we do things now is better. In the builders-and-programmers metaphor, civilization need not fear woodpeckers.<a href="https://www.tbray.org/ongoing/When/202x/2021/05/15/Testing-in-2021#p-4" class="plink">&nbsp;¶</a></p>

    <p>For example: In my years at Google and AWS, we had outages and failures, but very very few of them were due to
    anything as simple as a software bug. Botched deployments, throttling misconfigurations, cert problems (OMG cert problems), DNS
    hiccups, an intern doing a load test with a Python script, malfunctioning canaries, there are lots of branches in that
    trail of tears. But usually not just a bug.</p>

    <p>I can’t remember when precisely I became infected, but I can testify: Once you are,
    you’re never going to be comfortable in the presence of untested code.</p>

    <p>Yes, you could use a public toilet and not wash your
    hands. Yes, you could eat spaghetti with your fingers. But responsible adults just don’t do those things. Nor do they ship
    untested code. And by the way, I no longer hate software that I’ve been working on for a while.</p>

    <p>I became monotonically less tolerant of lousy testing with every year that went by. I blocked promotions, pulled rank,
    berated senior development managers, and was generally pig-headed. I can get away with this (mostly) without making enemies
    because I’m respectful and friendly and sympathetic.  But not, on this issue, flexible.</p>

    <p>So, here’s the hill I’ll die on (er, well, a range of foothills I guess):</p>

    <ol>
      <li><p>Unit tests are an essential investment in your software’s future.</p>
</li>
      <li><p>Test coverage data is useful and you should keep an eye on it.</p>
</li>
      <li><p>Untested legacy code bases can and should be improved incrementally</p>
</li>
      <li><p>Unit tests need to run <em>very</em> quickly with a single IDE key-combo, and it’s perfectly OK to run them every few
      seconds like a nervous tic.</p>
</li>
      <li><p>There’s no room for testing religions; do what works.</p>
</li>
      <li><p>Unit tests empower code reviewers.</p>
</li>
      <li><p>Integration tests are super important and super hard, particularly in a microservices context.</p>
</li>
      <li><p>Integration tests need to pass 100%, it’s not OK for there to be failures that are ignored.</p>
</li>
      <li><p>Integration tests need to run “fast enough“.</p>
</li>
      <li><p>It’s good for tests to include benchmarks.</p>
</li>
    </ol>
    <p>Now I’ll expand on the claims in that list. Some of them need no further defense (e.g. “unit tests should run fast”) and will get
    none. But first…</p>

    <p id="p-3" class="p1"><span class="h2">Can you prove it works?</span> · 
    Um, nope. I’ve looked around for high-quality research on testing efficacy, and didn’t find much.<a href="https://www.tbray.org/ongoing/When/202x/2021/05/15/Testing-in-2021#p-3" class="plink">&nbsp;¶</a></p>

    <p>Which shouldn’t be surprising.  You’d need to find two substantial teams doing nontrivial development tasks where there
    is rough-or-better equivalence in scale, structure, tooling, skill levels, and work
    practices<span class="dashes"> —</span> in everything but testing.  Then you’d need to study productivity and quality over a decade
    or longer.  As far as I know, nobody’s ever done this 
    and frankly, I’m not holding my breath. So we’re left with anecdata, what Nero Wolfe called “Intelligence informed by
    experience.”</p>

    <p>So let’s not kid ourselves that our software-testing tenets constitute scientific knowledge. But the world has other kinds of
    useful lessons, so let’s also not compromise on what our experience teaches us is right.</p>

    <p id="p-5" class="p1"><span class="h2">Unit tests matter now and later</span> · 
    When you’re creating a new feature and implementing a bunch of functions to do it, don’t kid yourself that you’re smart
    enough, in advance, to know which ones are going to be error-prone, which are going to be bottlenecks, and which ones are going
    to be hard for your successors to understand. <em>Nobody is smart enough!</em>  So write tests for everything that’s not a
    one-line accessor.<a href="https://www.tbray.org/ongoing/When/202x/2021/05/15/Testing-in-2021#p-5" class="plink">&nbsp;¶</a></p>

    <p>In case it’s not obvious, the graphic above from Spotify that dismisses  unit testing with the label “implementation detail”
    offends me.  I smell Architecture Astronautics here, people who think all the work is getting the boxes and arrows right on the
    whiteboard, and are above dirtying their hands with semicolons and <code>if</code> statements.  If your basic microservice code
    isn’t well-tested you’re building on sand.</p>

    <p>Working in a well-unit-tested codebase gives developers courage. If a little behavior change would benefit from
    re-implementing an API or two you can be bold, can go ahead and do it. Because with good unit tests, if you screw up, you’ll find
    out fast.</p>
 
    <p>And remember that code is read and updated way more often than it’s written.  I personally think that writing good tests
    helps the developer during the first development pass and doesn’t slow them down. But I <em>know</em>, as well as I know anything
    about this vocation, that unit tests give a major productivity and pain-reduction boost to the many subsequent developers who
    will be learning and revising this code.  That’s business value!</p>

    <p id="p-10" class="p1"><span class="h2">Exceptions</span> · 
    Where can we ease up on unit-test coverage?  Back in 2012 
    <a href="https://www.tbray.org/ongoing/When/201x/2011/12/27/Type-Systems#p-4">I wrote</a> about how testing UI code, and in particular mobile-UI code,
    is unreasonably hard, hard enough to probably not be a good investment in some cases.<a href="https://www.tbray.org/ongoing/When/202x/2021/05/15/Testing-in-2021#p-10" class="plink">&nbsp;¶</a></p>

    <p>Here’s another example, specific to the Java world, where in the presence of dependency-injection frameworks you have huge
    files with literally thousands of lines of config gibberish <i>[*cough* Spring Boot *cough*]</i> and life’s just too short.</p>

    <p>A certain number of exception-handling scenarios are so far-fetched that you’d expect your data center to be in flames before
    they happen, at which point an <code>IOException</code> is going to be the least of your troubles.  So maybe don’t obsess about
    those particular <code>if&nbsp;err&nbsp;!=&nbsp;nil</code> clauses.</p>

    <p id="p-6" class="p1"><span class="h2">Coverage data</span> · 
    I’m not dogmatic about any particular codebase hitting any particular coverage number. But the data is useful
    and you should pay attention to it.<a href="https://www.tbray.org/ongoing/When/202x/2021/05/15/Testing-in-2021#p-6" class="plink">&nbsp;¶</a></p>

    <p>First of all, look for anomalies: Files that have noticeably low (or high) coverage numbers. Look for changes between
    check-ins. </p>

    <p>And coverage data is more than just a percentage number. When I’m most of the way through some particular piece of
    programming, I like to do a test run with coverage on and then quickly glance at all the significant code chunks, looking at the
    green and red sidebars.  Every time I do this I get surprises, usually in the form of some file where I thought my unit
    tests were clever but there are huge gaps in the coverage.  This doesn’t just make me want to improve the testing, it teaches me
    something I didn’t know about how my code is reacting to inputs.</p>

    <p>Having said that, there are software groups I respect immensely who have hard coverage requirements and stick to them.
    There’s one at AWS that actually has a 100%-coverage blocking check in their CI/CD pipeline.  I’m not sure that’s reasonable, 
    but these people are doing very
    low-level code on a crucial chunk of infrastructure where it’s maybe reasonable to be unreasonable. Also they’re smarter than me.</p>

    <p id="p-7" class="p1"><span class="h2">Legacy code coverage</span> · 
    I have never, and mean <em>never</em>, worked with a group that wasn’t dragging along weakly-tested legacy code.
    Even a testing maniac like me isn’t going to ask anyone to retro-fit high-coverage unit testing onto
    that stinky stuff.<a href="https://www.tbray.org/ongoing/When/202x/2021/05/15/Testing-in-2021#p-7" class="plink">&nbsp;¶</a></p>

    <p>Here’s a policy I’ve seen applied successfully; It has two parts: First, when you make any significant change
    to a function that doesn’t have unit tests, write them.  Second, no check-in is allowed to make the coverage numbers go down.</p>

    <p>This works out well because, when you’re working with a big old code-base, updates don’t usually scatter
    uniformly around it; there are hot spots where useful behavior clusters. So if you apply this policy, the
    code’s “hot zone” will organically grow pretty good test coverage while the rest, which probably hasn’t been touched or looked
    at for years, is ignored, and that’s OK.</p>

    <p id="p-8" class="p1"><span class="h2">No religion</span> · 
    Testing should be an ultimately-pragmatic activity with no room for ideology.<a href="https://www.tbray.org/ongoing/When/202x/2021/05/15/Testing-in-2021#p-8" class="plink">&nbsp;¶</a></p>

    <p>Please don’t come at me with pedantic arm-waving about mocks vs stubs vs fakes; nobody cares.  On a related subject, when I
    discovered that lots of people were using
    <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBLocal.html">DynamoDB Local</a> in their unit
    tests
    for code that runs against DynamoDB, I was shocked. But hey, it works, it’s fast, and it’s a lot less hassle than either writing yet another mock or setting
    up a linkage to the actual cloud service. Don’t be dogmatic!</p>

    <p>Then there’s the TDD/BDD faith. Sometimes, for some people, it works fine. More power to ’em. It almost never
    works for me in a pure form, because my coding style tends to be chaotic in the early stages, I keep refactoring and refactoring
    the functions all the time.  If I knew what I wanted them to do before I started writing them, then TDD might make sense.  On
    the other hand, when I’ve got what I think is a reasonable set of methods sketched in and I’m writing tests for the basic code,
    I’ll charge ahead and write more for stuff that’s not there yet. Which doesn’t qualify me for a membership of the church of TDD
    but I don’t care.</p>

    <p>Here’s another religion: Java doesn’t make it easy to unit-test private methods. Java is wrong. Some people claim you
    shouldn’t want to test those methods because they’re not part of the class contract. Those people are wrong. It is perfectly
    reasonable to compromise encapsulation and make a method non-private just to facilitate testing.  Or to write an API to take an
    interface rather than a class object for the same reason.</p>

    <p>When you’re running a bunch of tests against a complicated API, it’s tempting to write a <code>runTest()</code> helper 
    that puts the arguments in the right shape and runs standardized checks against the results. If you don’t do this, you end up
    with a lot of repetitive cut-n-pasted code.</p>

    <p>There’s room for argument here, none for dogma.  I’m <em>usually</em> vaguely against doing this. Because
    when I change something and a unit test I’ve never seen before fails, I don’t want to have to go understand a bunch of helper
    routines before I can figure out what happened.</p>

    <p>Anyhow, if your engineers are producing code with effective tests, don’t be giving them any static about how it got
    that way.</p>

    <p id="p-9" class="p1"><span class="h2">The reviewer’s friend</span> · 
    Once I got a call out of the blue from a Very Important Person saying “Tim, I need a favor. The [REDACTED] group is
    spinning their wheels, they’re all fucked up. Can you have a look and see if you can help them?”  So I went over and introduced
    myself and we talked about the problems they were facing, which were tough.<a href="https://www.tbray.org/ongoing/When/202x/2021/05/15/Testing-in-2021#p-9" class="plink">&nbsp;¶</a></p>

    <p>Then I got them to show me the codebase and I pulled
    up a few review requests. The first few I looked at had no unit tests but did have notes saying “Unit
    tests to come later.”  I walked into their team room and said “People, we need to have a talk right now.”</p>

    <p>[Pause for a spoiler alert: The unit tests <em>never</em> come along later.]</p>

    <p>Here’s the point: The object of code reviewing is not correctness-checking. A reviewer is entitled to assume that the code
    works. The reviewer should be checking for O(N<sup>3</sup>) bottlenecks, readability problems, klunky
    function arguments, shaky error-handling, and so on.  It’s not fair to ask a reviewer to think about that stuff if you don’t
    have enough tests to demonstrate your code’s basic correctness.</p>

    <p>And it goes further. When I’m reviewing, it’s regularly the case that I have trouble figuring out what the hell the developer
    is trying to accomplish in some chunk of code or another. Maybe it’s appropriate to put in a review comment about
    readability? But first, I flip to the unit test and see what it’s doing, because sometimes that makes it obvious what the dev
    thought the function was for. This also works for subsequent devs who have to modify the code.</p>

    <p id="p-11" class="p1"><span class="h2">Integration testing</span> · 
    The people who made the pictures up above all seem to think it’s important. They’re right, of course. 
    I’m not sure the difference between “integration” and “end-to-end” matters, though.<a href="https://www.tbray.org/ongoing/When/202x/2021/05/15/Testing-in-2021#p-11" class="plink">&nbsp;¶</a></p>

    <p>The problem is that moving from
    monoliths to microservices, which makes these tests more important, also makes them harder to build.  Which is another good
    reason to stick with a nice simple monolith if you can. No, I’m not kidding.</p>

    <p>Which in turn means you have to be sure to budget time, including design and maintenance time, for 
    your integration testing. (Unit testing is just part of the basic coding budget.)</p>

    <p id="p-12" class="p1"><span class="h2">Complete and fast</span> · 
    I know I find these hard to write and I know I’m not alone because I’ve worked with otherwise-excellent teams who
    have crappy integration tests.  <a href="https://www.tbray.org/ongoing/When/202x/2021/05/15/Testing-in-2021#p-12" class="plink">&nbsp;¶</a></p>

    <p>One way they’re bad is that they take hours to run. This is hardly controversial enough to worth saying but, since it’s a
    target that’s often missed, let’s 
    say it:  Integration tests don’t need to be as quick as unit tests but they do need to be fast enough that it’s reasonable to
    run them every time you go to the bathroom or for coffee, or get interrupted by a chat window. Which, once again, is hard to
    achieve.</p>

    <p>Finally, time after time I see integration-test logs show failures and some dev says “oh yeah, those particular tests are
    flaky, they just fail sometimes.” For some reason they think this is OK.  Either the tests exercise something that might fail in
    production, in which case you should treat failures as blockers, or they don’t, in which case you should take them out of the damn
    test suite which will then run faster.</p>

    <p id="p-13" class="p1"><span class="h2">Benchmarks</span> · 
    Since I’ve almost always worked on super-performance-sensitive code, I often end up writing benchmarks, and after a while I got
    into the habit of leaving a few of them live in the test suite. Because I’ve observed more than a few outages caused by a
    performance regression, something as dumb as a config tweak pushing TLS compute out of hardware and into Java bytecodes.
    You’d really rather catch that kind of thing before you push.<a href="https://www.tbray.org/ongoing/When/202x/2021/05/15/Testing-in-2021#p-13" class="plink">&nbsp;¶</a></p>

    <p id="p-15" class="p1"><span class="h2">Tooling</span> · 
    There’s plenty. It’s good enough. Have your team agree on which they’re going to use and become expert in it. Then
    don’t blame tools for your shortcomings.<a href="https://www.tbray.org/ongoing/When/202x/2021/05/15/Testing-in-2021#p-15" class="plink">&nbsp;¶</a></p>

    <p id="p-14" class="p1"><span class="h2">Where we stand</span> · 
    The news is I think mostly good, because most sane organizations are starting to exhibit pretty good testing discipline,
    especially on server-side code.  And like I said, this old guy sees a lot less bugs in production code than there used to be.<a href="https://www.tbray.org/ongoing/When/202x/2021/05/15/Testing-in-2021#p-14" class="plink">&nbsp;¶</a></p>

    <p>And every team has to wrestle with those awful old stagnant pools of untested legacy. Suck it up; dealing with that is
    just part of the job.  Anyhow, you probably wrote some of it.</p>

    <p>But here and there every day, teams lose their way and start skipping the hand-wash after the toilet
    visit. Don’t. And don’t ship untested code.</p>

  <hr>
<div id="commentHere"><p>Comments on this fragment are closed.</p></div>
<div id="footer"><p class="footer"><b>Updated: 2021/06/01</b></p>
</div>
<hr>
<h2 id="comments">Contributions</h2>
<div class="comments"><p>Comment feed for <span class="o">ongoing</span>:<a href="https://www.tbray.org/ongoing/comments.atom"><img src="./ongoing by Tim Bray · Testing in the Twenties_files/Feed.png" alt="Comments feed"></a></p><div class="comment-a" id="c1622505726.846134"><p class="from">From: <a href="http://trsjs.48k.ca/">Peter Phillips</a> (May 31 2021, at 17:02)</p><p>How does Open Source score on testing?</p><p>I didn't look hard, but I don't recall GCC or Linux including test suites in their source.</p>
<p><i>[<a href="https://www.tbray.org/ongoing/When/202x/2021/05/15/Testing-in-2021#c1622505726.846134">link</a>]</i></p></div><div class="comment-b" id="c1622519749.549538"><p class="from">From: <a href="http://https//zackofalltrades.com">Zack</a> (May 31 2021, at 20:55)</p><p>@Peter Phillips - </p><p>Most open source that is consumed downstream by many people eventually has someone benchmarking and testing it.</p><p>Linux in particular has <a href="http://linux-test-project.github.io/">http://linux-test-project.github.io</a> and various 3rd parties running benchmarks on it: <a href="https://www.phoronix.com/scan.php?page=article&amp;item=linux-50-59">https://www.phoronix.com/scan.php?page=article&amp;item=linux-50-59</a></p><p>But open source as "random project on free code hosting" is always hit or miss.</p>
<p><i>[<a href="https://www.tbray.org/ongoing/When/202x/2021/05/15/Testing-in-2021#c1622519749.549538">link</a>]</i></p></div><div class="comment-a" id="c1622522815.433461"><p class="from">From: <a href="http://https//justin.abrah.ms">Justin Abrahms</a> (May 31 2021, at 21:46)</p><p>Responding to Peter's comment on this article, I think open source scores better than industry code. At least the open source code you're likely to depend on for your application.</p><p>I think the main reason for this is open source code is going to get drive-by pull requests. Your choices then become to take on code that may or may not work, invest time into manual tests, or require unit tests along with the submission.</p><p>As open source authors are grappling with the reality of the amount of thankless work the "maintenance" task has.. easing the burden on themselves just makes sense.</p>
<p><i>[<a href="https://www.tbray.org/ongoing/When/202x/2021/05/15/Testing-in-2021#c1622522815.433461">link</a>]</i></p></div><div class="comment-b" id="c1622524329.263457"><p class="from">From: <a href="http:">Ryan McNally</a> (May 31 2021, at 22:12)</p><p>We write tests for the same reason that we (should) write documentation - we can't understand the system the we've built. Hence the most useful tests are often the most readable ones (where readability can be a function of how the test is defined or the output that is produced when it runs).</p><p>Optimising your tests for readability will produce something extremely valuable: documentation that is always accurate. That's obviously not the end of the story for documentation, but it's a big help.</p><p>Regarding code coverage, I'm a big fan of mutation testing (e.g. pitest.org). It takes longer to run, but will give a much better picture of the health of the test suite.</p>
<p><i>[<a href="https://www.tbray.org/ongoing/When/202x/2021/05/15/Testing-in-2021#c1622524329.263457">link</a>]</i></p></div><div class="comment-a" id="c1622527968.234117"><p class="from">From: <a href="http://https//gioele.io">Gioele</a> (May 31 2021, at 23:12)</p><p>&gt; Peter Phillips</p><p>&gt;</p><p>&gt; I didn't look hard, but I don't recall GCC or Linux including test suites in their source.</p><p>GCC has a huge, awe-inspiring regression test suite. Pretty much all closed bugs have an attached test case.</p><p><a href="https://gcc.gnu.org/git/?p=gcc.git;a=tree;f=gcc/testsuite;h=44b6da89e232e452796191c702671c7a84da1e05;hb=HEAD">https://gcc.gnu.org/git/?p=gcc.git;a=tree;f=gcc/testsuite;h=44b6da89e232e452796191c702671c7a84da1e05;hb=HEAD</a></p>
<p><i>[<a href="https://www.tbray.org/ongoing/When/202x/2021/05/15/Testing-in-2021#c1622527968.234117">link</a>]</i></p></div><div class="comment-b" id="c1622536677.167006"><p class="from">From: <a href="http://https//matklad.github.io">matklad</a> (Jun 01 2021, at 01:37)</p><p>Somewhat amusingly, I've just wrote a post which argues for the opposite point regarding unit tests: <a href="https://matklad.github.io/2021/05/31/how-to-test.html">https://matklad.github.io/2021/05/31/how-to-test.html</a></p><p>It was very helpful to stumble upon this perfect compliment, thanks!</p>
<p><i>[<a href="https://www.tbray.org/ongoing/When/202x/2021/05/15/Testing-in-2021#c1622536677.167006">link</a>]</i></p></div><div class="comment-a" id="c1622575768.445614"><p class="from">From: Gil Tayar (Jun 01 2021, at 12:29)</p><p>As someone who's read Kent C. Dodds, and is (I think) familiar with his thinking, I have to say that both you and he are in agreement to the what, just not to the names. Given that I am NOT KCD, though, let me share what I think about unit vs integration vs e2e tess.</p><p>I believe that for you, integration tests are when you test multiple microservices together to test how they work in tandem. While for myself, integration tests are just higher level unit tests that integrate a few units and check them together, when a unit means a function, class, module, or whatever. So what I call integration tests, you, I believe, just think of as a unit test that tests multiple things together (in the same microservice). And as such, we prefer checking those units _together_ and not separately, because we prefer as minimal mocking as possible, and believe we can get more value out of the tests, while sacrificing some speed (much less than "go to the bathroom" speed).</p><p>What you call integration: tests that tests multiple microservices together, I call E2E, and we are all in agreement I believe that those are slow, flaky, and there should be very little of those. I actually _don't_ even do a single E2E test, and surprisingly, only testing each microservice in isolation works suprisingly well. The only E2E test I have is the one testing production after a deploy to see that I haven't screwed up big time.</p>
<p><i>[<a href="https://www.tbray.org/ongoing/When/202x/2021/05/15/Testing-in-2021#c1622575768.445614">link</a>]</i></p></div><div class="comment-b" id="c1622579790.996764"><p class="from">From: <a href="http:">Jim</a> (Jun 01 2021, at 13:36)</p><p>Nice article. I'd say I have 2 peeves though. </p><p>One is your comments about TDD. No offence, but I've seen similar remarks a lot, and you're a bit offbase there. The reason you don't TDD early on is precisely the same reason I'm pretty strict about it. I've noticed that (the more reasonable of) those of us that are, treat TDD mostly as a design exercise, not a testing one. We just get tests as a bonus. And of course, sometimes the tests need to change, and some I throw away. Your perspective sounds like that of many other people I've worked with who are looking at it wrong. Something to consider, maybe.</p><p>I also do "exploratory testing" when trying out new libraries, techniques etc, especially in spikes. I find it helps to keep the crap tests then, as a warning for later :-)</p><p>Secondly, I think needing to make private methods public in order to test them is a code smell. Almost all the time, there is another class trying to get out. Somewhat more rarely, there is code in private methods that cannot be exercised by calling the public ones, which means there is unnecessary code in the private ones, and it can safely be removed. It's not that I've never done this, but I can't remember how long ago the last time was.</p><p>Everything else I think is very well expressed.</p>
<p><i>[<a href="https://www.tbray.org/ongoing/When/202x/2021/05/15/Testing-in-2021#c1622579790.996764">link</a>]</i></p></div><div class="comment-a" id="c1622609240.191986"><p class="from">From: <a href="http://thien.fun/">Thien</a> (Jun 01 2021, at 21:47)</p><p>@Jim: I have very similar thoughts to yours while reading this great article from Tim.</p><p>We should think of TDD as a helpful practice (for some people) to create well-designed, well-tested code instead of considering it "the best and only way to archive self-testing code."</p><p>I do make private methods/properties internal to test their internal state sometimes because it's easier that way. However, I have to think about those cases a lot and also consider them code smells.</p><p>Thanks a lot for your contribution :)</p>
<p><i>[<a href="https://www.tbray.org/ongoing/When/202x/2021/05/15/Testing-in-2021#c1622609240.191986">link</a>]</i></p></div><div class="comment-b" id="c1622646058.148429"><p class="from">From: <a href="http:">Glyn Normington</a> (Jun 02 2021, at 08:00)</p><p>Good article Tim!</p><p>On TDD, I once worked in an XP team developing Linux container code. The codebase cycled slowly from using one layered file system to the next and eventually back again. Sometimes a bit of thoughtful design really is necessary, particularly when working in a codebase with few architectural precedents (NB. unlike most web applications, where TDD is most prevalent).</p><p>On unit testing, I remember writing unit tests in the 1980's, but then deleting them (!) so as not to have to maintain them. The problem was lack of automation, i.e. an easy way to re-run tests plus CI to automate this. Now there are no (valid) excuses for skipping unit testing </p>
<p><i>[<a href="https://www.tbray.org/ongoing/When/202x/2021/05/15/Testing-in-2021#c1622646058.148429">link</a>]</i></p></div><div class="comment-a" id="c1622695224.388541"><p class="from">From: <a href="http://https//kentcdodds.com">Kent C. Dodds</a> (Jun 02 2021, at 21:40)</p><p>Hi, I'm the creator of the Testing Trophy. Gil Taylor left a comment already that sums up my own response well (<a href="https://www.tbray.org/ongoing/When/202x/2021/05/15/Testing-in-2021#c1622575768.445614">https://www.tbray.org/ongoing/When/202x/2021/05/15/Testing-in-2021#c1622575768.445614</a>). We are largely in agreement and our apartment disagreement in the shape of testing classification is due to our differing definitions of the categories which is an unfortunate problem in the testing world at large (as noted in Martin's post which referenced this one: <a href="https://martinfowler.com/articles/2021-test-shapes.html).">https://martinfowler.com/articles/2021-test-shapes.html).</a></p><p>I appreciate you taking the time to share your perspective on this and I may write my own blog post to clear up the confusion that I contributed to this whole thing 😅</p><p>As Justin Searls said, the classifications are largely a distraction from where our focus should be. So thank you for your efforts at bringing the focus to where it should be!</p>
<p><i>[<a href="https://www.tbray.org/ongoing/When/202x/2021/05/15/Testing-in-2021#c1622695224.388541">link</a>]</i></p></div><div class="comment-b" id="c1622789070.720819"><p class="from">From: Enrico (Jun 03 2021, at 23:44)</p><p>"Why did you add FunctionFoo? You should have just refactored a bit FunctionBar".</p><p>"I started changing FunctionBar, but then I had 10 tests failing. I did not have time to understand and fix all of them, so I created FunctionFoo"</p><p>Since the late '90s I have heard this kind of conversation a few times. And this conversation highlights situations where not well crafted tests become an obstacle to refactoring, rather then being a precondition.</p><p>Well crafted tests, i.e. tests that test stable interfaces and not implementation details, are the ones that we need.</p><p>Similarly I think we should always think twice before adding mocks/fake/stubs. Any of these is adding a risk. The risk is "what happens when I remove it?"</p><p>So I think that the more we can do integrated tests the better it is, as long as they run fast enough and are not flaky, as Tim and others point out. But please, be careful when you preach for Unit Tests everywhere. People with less experience take it as a dogma. I have seen tests testing the sql query, I mean the string representing the query with a mocked DB, because "runing against a real DB and checking the result of the real query was NOT A UNIT TEST".</p>
<p><i>[<a href="https://www.tbray.org/ongoing/When/202x/2021/05/15/Testing-in-2021#c1622789070.720819">link</a>]</i></p></div></div></div>

<div id="rightcontent"><div class="oo"><a id="to-home" href="https://www.tbray.org/ongoing/"><span id="home">ongoing</span></a></div>
<div>
<div class="principles">
<a href="https://www.tbray.org/ongoing/WhatItIs">What this is</a> ·
<a href="https://www.tbray.org/ongoing/ongoing.atom"><img title="Subscribe to ongoing" alt="Subscribe to ongoing" src="./ongoing by Tim Bray · Testing in the Twenties_files/Feed.png"></a><br>
<a href="https://www.tbray.org/ongoing/Truth">Truth</a> ·
<a href="https://www.tbray.org/ongoing/Biz">Biz</a> ·
<a href="https://www.tbray.org/ongoing/Tech">Tech</a></div>
<a href="https://www.tbray.org/ongoing/misc/Tim">author</a> ·
<a href="http://www.textuality.com/BillBray/">Dad</a><br>
<a href="https://www.tbray.org/ongoing/misc/Colophon">colophon</a> ·
<a href="https://www.tbray.org/ongoing/misc/Copyright">rights</a>
</div>
<hr>
<div id="cats">
<a href="https://www.tbray.org/ongoing/When/202x/2021/05/">May</a> <a href="https://www.tbray.org/ongoing/When/202x/2021/05/15/">15</a>, <a href="https://www.tbray.org/ongoing/When/202x/2021/">2021</a><br> · <a href="https://www.tbray.org/ongoing/What/Technology">Technology</a><span class="more"> (90 fragments)</span>
<br> · · <a href="https://www.tbray.org/ongoing/What/Technology/Software">Software</a><span class="more"> (80 more)</span>
</div>

<div class="employ">
<p>By <a rel="author" href="https://www.tbray.org/ongoing/misc/Tim">Tim Bray</a>.</p>
<p>The opinions expressed here <br>
are my own, and no other party<br>
necessarily agrees with them.</p>
<p>A full disclosure of my<br>
professional interests is<br> 
on the <a href="https://www.tbray.org/ongoing/misc/Tim">author</a> page.</p>
<p>I’m on <a rel="me" href="https://cosocial.ca/@timbray">Mastodon</a>!</p>
</div>



</div>
</div>
</div>



</body></html>